---
title: "Projeto Final AEM II"
author: "André Dambry, Mainara Cardoso, Tiago Pardo"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objetivo:

### 1) Utilizar métodos de aprendizagem supervisionada (regressão ou classificação)

### 2) Realização de análise de um problema não supervisionado

## Base de dados:

### Marketing Analytics

<https://www.kaggle.com/datasets/jackdaoud/marketing-data/data?select=dictionary.png>

A base de dados contém informações de 2206 clientes de uma empresa. Os dados são relacionados à:

-   Perfil dos clientes

-   Produtos

-   Campanhas bem / mau sucedidas

-   Performance do canal

# Proposta de Valor

Através dos modelos supervisionados, nossa proposta é dentificar chance de clientes aderirem às campanhas.

Com isto feito, nossa proposta, através do método KMeans será realizar o agrupamento de clientes. Isso é útil para que a realização de segmentação de público em campanhas de marketing.

## Modelos supervisionados:

Tipo de Problema: Classificação

Modelos:

-   Regressão Logística

-   Floresta Aleatória

-   Boosting

-   Lasso

-   Ridge

-   Rede Neural

## Modelos não supervisionados:

-   KMeans

# Bibliotecas

```{r}
#install.packages("summarytools")
#install.packages("reticulate")
#install.packages("keras")
#install.packages("imbalance")
#install.packages("DMwR")
```

```{r}
library (readr)
library (dplyr)
library (tidyr)
library (tidyverse)
library (skimr)
library (ggplot2)
library (summarytools)
library (tidymodels)
library (ISLR)
library (doParallel)
library (glmnet)
library (ranger)
library (parsnip)
library (vip)
library(keras)
library(corrplot)
library(imbalance)
library(DMwR)
 #library(tensorflow) 
```

# Importação de dados

```{r}
urlfile <- "https://raw.githubusercontent.com/nailson/ifood-data-business-analyst-test/master/ifood_df.csv"
dados <- read_csv(url(urlfile))
```
# Geração de metadados

```{r}
metadata <- dados  %>%
  lapply(type_sum) %>%
  as_tibble() %>%
  pivot_longer(cols = 1:ncol(dados),
               names_to = "Coluna",
               values_to = "Tipo") %>%
  inner_join(
    dados %>%
      summarise(across(everything(), ~sum(is.na(.)))) %>%
      pivot_longer(cols = 1:ncol(dados),
                   names_to = "Coluna",
                   values_to = "Total NA")
  )

metadata
```

# 1. Tratamento

```{r}
summary(dados)
```

Verificando ausência de valores

```{r}
colSums(is.na(dados))
```
A base de dados não possui valores ausentes.

Verificando o tipo de dados de cada coluna. 

```{r}
sapply(dados, class)
```
Observa-se que todos as colunas são do tipo numérica. Isso ocorre porque as variáveis categóricas, como estado civil (marital) e nível de educação (education), já estão convertidas para dummy.

Verificando valores negativos.

```{r}
sapply(dados, function(x) sum(x < 0))

```
A variável "MntRegularProds", que representa os gastos com produtos regulares, é a única que apresenta valores negativos. 

Verificando colunas com valores únicos.

```{r}
col_valores_unicos <- sapply(dados, function(x) length(unique(x)) == 1)
names(dados)[col_valores_unicos]
```

As colunas "Z_CostContact" e "Z_Revenue", que representam, respectivamente, o custo do contato e o valor da receita, possuem um único valor em todas as observações. 

Assim, pontos a serem destacados sobre a análise inicial dos dados.

1. Ausência de valores vazios.
2. Tipo dos dados estão corretos.
3. Variáveis categóricas estão previamente tratadas como dummies.
4. A coluna "MntRegularProds" possui valores negativos.
5. As colunas "Z_CostContact" e "Z_Revenue" possuem um único valor em todas as observações. 

## 1.1. Limpeza

Na análise realizada, indentificamos que a variável "MntRegularProds", que indica o valor das compras efetuadas na plataforma, possui 3 valores negativos. Considerando que os valores das compras não podem ser negativos por natureza e que o número de observações com essa característica é pequeno, optamos por remover esses dados da base.

```{r}
# filtrando valores negativos da base
dados <- dados %>%
            filter (dados$MntRegularProds > 0)
```

As colunas "Z_CostContact" e "Z_Revenue", que representam, respectivamente, o custo do contato e o valor da receita, possuem um único valor em todas as observações. Logo, essas colunas não contribuem com informações úteis para aprimorar a precisão dos modelos preditivos ou da segmentação dos clientes. Portanto, estas variáveis serão excluídas da base.

```{r}
col_rem <- c("Z_CostContact", "Z_Revenue")

dados <- dados %>% select(-one_of(col_rem))
```

```{r}
dados
```

## 1.2. Análise de outliers

Para melhor funcionamento dos modelos, verificaremos a existência de outliers nas colunas com valores numéricos.

Obs: Não consideramos colunas com valores numéricos binários ou que não façam sentido de terem seus outliers analisados.

Verificando colunas com dados binários. 

```{r}
colunas_binarias <- sapply(dados, function(x) length(unique(x)) == 2)
names(dados)[colunas_binarias]
```
```{r}
# Selecionar colunas não-binárias
colunas_nao_binarias <- names(dados)[!colunas_binarias]

# Cálculo de outliers

for (coluna in colunas_nao_binarias) {
  q1 <- quantile(dados[[coluna]], 0.25)
  q3 <- quantile(dados[[coluna]], 0.75)
  q95 <- quantile(dados[[coluna]], 0.95)
  iqr <- q3 - q1
  median <- median(dados[[coluna]])
  outlier_limit <- median + 1.5 * iqr
  # Count the number of values above the limit
  outlier_count <- sum(dados[[coluna]] > outlier_limit)
  cat(sprintf('%20s | median+1.5xiqr: %8.2f | 95quantile: %8.2f | outliers: %d\n', 
              coluna, outlier_limit, q95, outlier_count))
}
```

Verificando distribuição dos dados com gráficos Bloxplot

```{r}
# Configuração de layout para exibir 4x5 boxplots
par(mfrow = c(4, 5), mar = c(2, 2, 2, 2))

#Plot dos boxplots
for (coluna in colunas_nao_binarias) {  
  boxplot(dados[[coluna]], main = coluna, coluna = "lightblue", border = "black", notch = TRUE)
}
```

# 2. Análise Exploratória de Dados

```{r}
# Contagem de frequências para cada campanha
cat("AcceptedCmp1\n")
print(table(dados$AcceptedCmp1))

cat("\nAcceptedCmp2\n")
print(table(dados$AcceptedCmp2))

cat("\nAcceptedCmp3\n")
print(table(dados$AcceptedCmp3))

cat("\nAcceptedCmp4\n")
print(table(dados$AcceptedCmp4))

cat("\nAcceptedCmp5\n")
print(table(dados$AcceptedCmp5))

cat("\nResponse\n")
print(table(dados$Response))

```
```{r}


```

Conforme observado, em todas as campanhas realizadas, o percentual de aceitação da campanha é muito inferior ao percentual de não aceitação, inclusive no caso da variável "Response". Esse desbalanceamento dos dados pode ocasionar problemas na previsão do modelo, visto que o modelo pode apresentar um erro maior na classificação da classe minoritária e, ainda assim, apresentar uma alta acurácia do modelo. 

Dessa forma, é importante entender o efeito do falso positivo e do falso negativo para o cenário analisado. Como o objetivo da nossa previsão é decidir se compensa enviar ou não a sexta campanha de marketing para um cliente, então as consequências de uma classificação errada para o cliente são:

- Falso negativo, que seria uma previsão errada de que um cliente não vai aceitar a campanha, é um custo de oportunidade.
- Falso positivo, que seria uma previsão errada de que um cliente vai aceitar a campanha, implica no custo do envio da proposta de marketing.


# Tratamento do deslabalanceamento de dados

# Tentativa 1: 

Oversampling: 

Majority Weighted Minority Oversampling TEchnique


```{r}
#dados$Response <- as.factor(dados$Response)

```

```{r}
#sapply(dados, class)


```



```{r}
# instalar e carregar o pacote 'DMwR' que contém a função SMOTE
# install.packages("DMwR")
# instalar o pacote smotefamily
#install.packages("smotefamily")
#library(smotefamily)
```

```{r}

# Aplicar SMOTE
#dados_smote <- SMOTE(Response ~ ., dados, perc.over = 100*(1867/333 - 1))


```

## 2. Separação entre treino e teste

```{r}
split <- initial_split(dados, prop = 0.7, strata = "Response")

treinamento <- training(split)
teste <- testing(split)
```

# 3. Modelagem Estatística

Utilizaremos Tidy Models para os modelos que o comportam

```{r}
receita <- recipe(Response ~ ., treinamento) %>% # define a receita, com a variavel resposta e os dados de treinamento
  step_mutate(Response = as.factor(Response)) %>% 
  step_normalize(all_numeric())  #normaliza todas variaveis numericas
  #step_normalize(all_of(c("MntTotal", "MntRegularProds", "MntWines", 
  #                                "MntFruits", "MntMeatProducts", "MntFishProducts", 
  #                                "MntSweetProducts", "MntGoldProds", "NumDealsPurchases",
  #                                "NumCatalogPurchases", "NumStorePurchases", "NumWebPurchases")))  # normaliza as variaveis não binárias com maior número de outliers
# transformando variável resposta em fator

```

```{r}
receita_prep <- prep(receita) # prepara a receita definida acima

treinamento_proc <- bake(receita_prep, new_data = NULL) # obtem os dados de treinamento processados

teste_proc <- bake(receita_prep, new_data = teste) # obtem os dados de teste processados
```

## 3.1. Modelos Supervisionados

# 3.1.1. Regressão Logística

Definindo modelo, engine e realizando o fit
```{r}
fit_glm <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification") %>% 
  fit(Response ~ ., treinamento_proc) 


tidy(fit_glm) # estimativas do modelo ajustado em formato tidy

```

Criando Tibble com resultado

```{r}
fitted <- fit_glm %>% 
  predict(new_data = teste_proc, type = "prob") %>% # realiza predicao para os dados de teste
  mutate(observado = teste_proc$Response, # cria uma coluna com o valor observado de default
         modelo = "logistica") # cria uma coluna para indicar qual o modelo ajustado

head(fitted) # mostra as 6 primeiras linhas do tibble criado
```

### 2.1.2. LASSO

Criando o modelo

```{r}
lasso <- logistic_reg(penalty = tune(), mixture = 1) %>% # define o modelo lasso e o parametro a ser tunado (o lambda)
  set_engine("glmnet") %>% # define a engine do modelo
  set_mode("classification") # define que e'  problema de classificacao
```


Validação Cruzada

```{r}
set.seed(321)

cv_split <- vfold_cv(treinamento, v = 10, strata = "Response")

doParallel::registerDoParallel() # paraleliza os proximos comandos

lambda_tune <- tune_grid(lasso, # especificacao do modelo
                         receita,# a receita a ser aplicada a cada lote
                         resamples = cv_split, # os lotes da validacao cruzada
                         grid = 30,# quantas combinacoes de parametros vamos considerar
                         metrics = metric_set(roc_auc, accuracy)) # metricas consideradas

autoplot(lambda_tune) # plota os resultados

```

```{r}
lambda_tune %>% 
  collect_metrics() # obtem as metricas calculadas

best <- lambda_tune %>% 
  select_best("roc_auc") # seleciona a melhor combinacao de hiperparametros

fit_lasso <- finalize_model(lasso, parameters = best) %>% # informa os valores de hiperparametros a serem considerados
  fit(Response ~ .,data = treinamento_proc) # executa o modelo com os valores de hiperparametros definidos acima
```


```{r}
fitted <- fitted %>% #      empilha as previsoes do lasso
  bind_rows(fit_lasso %>% # os valores preditos pelo lasso
              predict(new_data = teste_proc, type = "prob") %>% 
              mutate(observado = teste_proc$Response, 
                     modelo = "lasso"))

head(fitted)
tail(fitted)
```

### 2.1.3. Ridge

Definindo o modelo, parãmetro de tunagem e engine

```{r}
ridge <- logistic_reg(penalty = tune(), mixture = 0) %>%
  set_engine("glmnet") %>%
  set_mode("classification") 
```

Validação Cruzada

```{r}
set.seed(321)

cv_split <- vfold_cv(treinamento, v = 10, strata = "Response")

doParallel::registerDoParallel() # paraleliza os proximos comandos

lambda_tune <- tune_grid(ridge, # especificacao do modelo
                         receita,# a receita a ser aplicada a cada lote
                         resamples = cv_split, # os lotes da validacao cruzada
                         grid = 30,# quantas combinacoes de parametros vamos considerar
                         metrics = metric_set(roc_auc, accuracy)) # metricas consideradas

autoplot(lambda_tune) # plota os resultados

```

```{r}
lambda_tune %>% 
  collect_metrics() # obtem as metricas calculadas

best <- lambda_tune %>% 
  select_best("roc_auc") # seleciona a melhor combinacao de hiperparametros

fit_ridge <- finalize_model(ridge, parameters = best) %>% # informa os valores de hiperparametros a serem considerados
  fit(Response ~ .,data = treinamento_proc) # executa o modelo com os valores de hiperparametros definidos acima

fitted <- fitted %>% #      empilha as previsoes do lasso
  bind_rows(fit_ridge %>% # os valores preditos pelo lasso
              predict(new_data = teste_proc, type = "prob") %>% 
              mutate(observado = teste_proc$Response, 
                     modelo = "ridge"))

head(fitted)
tail(fitted)
```

# 3.1.4. Floresta Aleatória

Definindo o modelo, cálculo de importância das variáveis e tipo de previsão

```{r}

rf <- rand_forest() %>%
  set_engine("ranger",
             importance = "permutation") %>%  
  set_mode("classification")

rf

```

Ajuste do modelo

```{r}
rf_fit <- rf %>% 
  fit(Response ~ ., treinamento_proc)
rf_fit
```

Verificando a importância das variaveis para o modelo

```{r}
vip(rf_fit)
```

Inserindo resultados na tabela

```{r}
fitted_rf <- rf_fit %>% 
  predict(new_data = teste_proc, type = 'prob') %>% # realiza predicao para os dados de teste
  mutate(observado = teste_proc$Response, # mesma estrutura do fitted_lm
         modelo = "random forest")
```


```{r}
fitted <- fitted %>% 
  bind_rows(fitted) # empilha o tibble fitted_rf abaixo do fitted_lm

head(fitted)
tail(fitted)
```

Avaliar desempenho

Ajustando Hiperparâmetros

```{r}
rf2 <- rand_forest(mtry = tune(),
                   trees = tune(), # todos argumentos com tune() serao tunados a seguir  
                   min_n = tune()) %>% 
  set_engine("ranger") %>% #
  set_mode("classification") 
  
rf2
```

Realizando Validação Cruzada

```{r}
set.seed(123)
cv_split <- vfold_cv(treinamento, v = 10)

registerDoParallel() # pararaleliza o processo

# para tunar os parametros
rf_grid <- tune_grid(rf2, # especificacao do modelo
                     receita, # a receita a ser aplicada a cada lote
                     resamples = cv_split, # os lotes da validacao cruzada
                     grid = 10, # quantas combinacoes de parametros vamos considerar
                     metrics = metric_set(roc_auc, accuracy))
```

Plotando os resultados

```{r}
autoplot(rf_grid)
```

Selecionando a melhor combinação e hiperparametros e salvando na variável

```{r}
rf_grid %>% 
  collect_metrics() 

best <- rf_grid %>% 
  select_best("roc_auc")
```

Finalizando o modelo

```{r}
rf_fit2 <- finalize_model(rf2, parameters = best) %>% # informa os valores de hiperparametros a serem considerados
  
  fit(Response ~ ., treinamento_proc) # executa o modelo com os valores de hiperparametros definidos acima
```

Previsão para os dados de teste

```{r}
fitted_rf2 <- rf_fit2 %>% # faz previsao para os dados de teste
  predict(new_data = teste_proc, type = 'prob') %>% 
  mutate(observado = teste_proc$Response, 
         modelo = "random forest - tune")

fitted <- fitted %>%
  bind_rows(fitted_rf2)
```

<font color=red> esse deve ser o último snipet após realizar a tabela de resultado de todos os modelos <font>


Redes neurais

```{r}

# Preparando dados

# Selecionando variáveis explicativas do treinamento e convertendo dados para matriz
X_trn <- treinamento %>% 
  select(-Response) %>% 
  as.matrix()
```

```{r}
# Selecionando variáveis explicativas do teste e convertendo dados para matriz
X_tst <- teste %>% 
  select(-Response) %>% 
  as.matrix()
```

```{r}

# Padroniza os dados de treinamento
X_trn <- scale(X_trn)

# Padroniza os dados de teste usando os mesmos parâmetros de centralização e escala dos dados de treinamento
X_tst <- scale(X_tst,
               center = attr(X_trn, "scaled:center"),
               scale = attr(X_trn, "scaled:scale"))

# Cria um modelo sequencial
net <- keras_model_sequential() %>% 
    layer_dense(units = 32, activation = "relu", input_shape = ncol(X_trn)) %>% # número de características de entrada
    #layer_dropout(rate=0.4) %>%  
    layer_dense(units = 16, activation = "relu") %>%
    #layer_dropout(rate=0.3) %>%  
    layer_dense(units = 8, activation = "relu") %>%
    layer_dense(units = 1, activation = "sigmoid") 

```

```{r}
library(tensorflow)
library(keras)
```



```{r}
# Modelo para classificação
net <- compile(net, 
               loss = "binary_crossentropy", 
               optimizer = "adam", 
               metrics = list(metric_recall())) 

# Treino dwo modelo para classificação
history_class <- fit(
  net, X_trn, treinamento$Response, 
  batch_size = 16, epochs = 20, 
  validation_split = 0.2)

```


Avaliando o modelo

```{r}
# Fazendo previsões no conjunto de teste
y_pred_prob <- predict(net, X_tst)

# Convertendo as probabilidades de previsão para classificações binárias
y_pred_class <- ifelse(y_pred_prob > 0.5, 1, 0)

# Calculando o erro
error_rate <- mean(y_pred_class != teste$Response)

conf_mat <- confusionMatrix(data = as.factor(y_pred_class), reference = as.factor(teste$Response))

# Calculando sensibilidade e especificidade diretamente
sensitivity <- caret::sensitivity(as.factor(y_pred_class), as.factor(teste$Response))
specificity <- caret::specificity(as.factor(y_pred_class), as.factor(teste$Response))
accuracy <- sum(diag(conf_mat$table)) / sum(conf_mat$table)

# Exibindo os resultados
print(paste("Erro: ", error_rate))
print(paste("Sensibilidade: ", sensitivity))
print(paste("Especificidade: ", specificity))
print(paste("Acurácia: ", accuracy))

```
```{r}
conf_mat

```








Comparando desempenho dos modelos

```{r}
fitted %>% 
  group_by(modelo) %>% 
  roc_auc(observado, .pred_1)

```
```{r}
fitted %>%
  mutate(
    observado_bin = as.numeric(observado) - 1, # Converte o fator para numérico (0 e 1)
    pred_bin = if_else(.pred_1 > 0.5, 1, 0)    # Cria previsões binárias
  ) %>%
  group_by(modelo) %>%
  summarise(
    auc = roc_auc(observado_bin, .pred_1)$.estimate,       # AUC-ROC
    accuracy = accuracy(observado_bin, pred_bin)$.estimate, # Acurácia
    sens = sens(observado_bin, pred_bin)$.estimate,         # Sensibilidade
    spec = spec(observado_bin, pred_bin)$.estimate,         # Especificidade
    ppv = ppv(observado_bin, pred_bin)$.estimate,           # Valor Preditivo Positivo
    npv = npv(observado_bin, pred_bin)$.estimate            # Valor Preditivo Negativo
  )

```